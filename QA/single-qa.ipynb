{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# @title inference function\n",
    "def inference(model,\n",
    "              processor,\n",
    "              image_path,\n",
    "              prompt,\n",
    "              sys_prompt=\"You are a helpful assistant.\",\n",
    "              max_new_tokens=2048,\n",
    "              return_input=False):\n",
    "    image = Image.open(image_path)\n",
    "    image_local_path = \"file://\" + image_path\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"image\": image_local_path},\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(\"text:\", text)\n",
    "    # image_inputs, video_inputs = process_vision_info([messages])\n",
    "    inputs = processor(text=[text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    # overall_text = processor.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    # print(\"overall_text:\", overall_text)\n",
    "    if return_input:\n",
    "        return output_text[0], inputs\n",
    "    else:\n",
    "        return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"\"\"\n",
    "/local_data/projects/vlm/QA/structured-data/1d914f73a4a243c3acac50d24f083aac/1533202427548877/CAM_FRONT_raw.jpg\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the distance between the trash compactor truck and the ego camera at frame 0 ? choose the closest one among the following options. \\nA. 62.6\\nB. 71.99\\nC. 81.38\\nD. 90.77\\nE. 100.16\\n (just reply the correct option's letter in json {'ans': ans}, unit in meters, frame idx starts from 0)\n",
    "\"\"\".strip()\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image.thumbnail([640,640], Image.Resampling.LANCZOS)\n",
    "display(image)\n",
    "\n",
    "## Use a local HuggingFace model to inference.\n",
    "response = inference(model, processor, image_path, prompt, max_new_tokens=128)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
